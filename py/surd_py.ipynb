{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84e914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations as icmb\n",
    "from typing import Tuple, Dict, Optional\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "class InfoCausality:\n",
    "    \"\"\"\n",
    "    InfoCausality class for Information-Theoretic Causality Analysis.\n",
    "    Supports:\n",
    "        - Shannon entropy\n",
    "        - Mutual information\n",
    "        - Conditional mutual information\n",
    "        - Transfer entropy\n",
    "        - SURD decomposition (Synergy, Unique, Redundancy)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x: np.ndarray, nbins: int = 8):\n",
    "        \"\"\"\n",
    "        Initialize InfoCausality class from raw target-agents original-lagged data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            2D array where:\n",
    "                - First column is the target variable (future)\n",
    "                - Remaining columns are agent (predictor) variables.\n",
    "        nbins : int\n",
    "            Number of bins (states) per variable dimension for discretization.\n",
    "        \"\"\"\n",
    "        self.p = self.create_pfm(x, nbins)\n",
    "        self.Ntot = self.p.ndim\n",
    "        self.Nvars = self.Ntot - 1\n",
    "        self.Nt = self.p.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def create_pfm(x: np.ndarray, nbins: int) -> np.ndarray:\n",
    "        \"\"\"Create joint probability frequency matrix.\"\"\"\n",
    "        hist, _ = np.histogramdd(x, bins=nbins)\n",
    "        hist = np.maximum(hist, 1e-14)\n",
    "        hist /= hist.sum()\n",
    "        return hist\n",
    "\n",
    "    @staticmethod\n",
    "    def mylog(x: np.ndarray) -> np.ndarray:\n",
    "        valid = (x > 0) & np.isfinite(x)\n",
    "        res = np.zeros_like(x)\n",
    "        res[valid] = np.log2(x[valid])\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(p: np.ndarray) -> float:\n",
    "        return -np.sum(p * InfoCausality.mylog(p))\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy_nvars(p: np.ndarray, indices) -> float:\n",
    "        excluded = tuple(set(range(p.ndim)) - set(indices))\n",
    "        marginalized = p if not excluded else p.sum(axis=excluded)\n",
    "        return InfoCausality.entropy(marginalized)\n",
    "\n",
    "    @staticmethod\n",
    "    def cond_entropy(p: np.ndarray, target_indices, conditioning_indices) -> float:\n",
    "        joint = InfoCausality.entropy_nvars(p, tuple(set(target_indices) | set(conditioning_indices)))\n",
    "        cond = InfoCausality.entropy_nvars(p, conditioning_indices)\n",
    "        return joint - cond\n",
    "\n",
    "    @staticmethod\n",
    "    def mutual_info(p: np.ndarray, set1_indices, set2_indices) -> float:\n",
    "        h1 = InfoCausality.entropy_nvars(p, set1_indices)\n",
    "        cond_h = InfoCausality.cond_entropy(p, set1_indices, set2_indices)\n",
    "        return h1 - cond_h\n",
    "\n",
    "    @staticmethod\n",
    "    def cond_mutual_info(p: np.ndarray, ind1, ind2, ind3) -> float:\n",
    "        combined = tuple(set(ind2) | set(ind3))\n",
    "        return InfoCausality.cond_entropy(p, ind1, ind3) - InfoCausality.cond_entropy(p, ind1, combined)\n",
    "\n",
    "    def transfer_entropy(self) -> np.ndarray:\n",
    "        num_vars = self.Nvars\n",
    "        TE = np.zeros(num_vars)\n",
    "        for i in range(1, num_vars + 1):\n",
    "            present_indices = tuple(range(1, num_vars + 1))\n",
    "            conditioning_indices = tuple(j for j in range(1, num_vars + 1) if j != i)\n",
    "            cond_ent_past = InfoCausality.cond_entropy(self.p, (0,), conditioning_indices)\n",
    "            cond_ent_past_input = InfoCausality.cond_entropy(self.p, (0,), present_indices)\n",
    "            TE[i - 1] = cond_ent_past - cond_ent_past_input\n",
    "        return TE\n",
    "\n",
    "    # =====================================================\n",
    "    # SURD decomposition (with parallel computation support)\n",
    "    # =====================================================\n",
    "    def surd(\n",
    "        self,\n",
    "        max_combs: Optional[int] = None,\n",
    "        n_jobs: int = 1,\n",
    "        backend: str = \"loky\",\n",
    "    ) -> Tuple[Dict, Dict, Dict, float]:\n",
    "        \"\"\"\n",
    "        Compute unified SURD decomposition, optionally with parallelization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_combs : int or None\n",
    "            Maximum combination order for high-dimensional synergy computation.\n",
    "            If None, standard SURD decomposition is used.\n",
    "        n_jobs : int\n",
    "            Number of parallel jobs (default 1 = no parallel).\n",
    "        backend : str\n",
    "            Joblib backend: 'loky', 'threading', or 'multiprocessing'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        I_R : dict\n",
    "            Redundancy / unique information for each combination.\n",
    "        I_S : dict\n",
    "            Synergy information for each combination.\n",
    "        MI : dict\n",
    "            Mutual information for each combination.\n",
    "        info_leak : float\n",
    "            Information leak ratio Hc/H.\n",
    "        \"\"\"\n",
    "        p = self.p\n",
    "        H = self.entropy_nvars(p, (0,))\n",
    "        Hc = self.cond_entropy(p, (0,), range(1, self.Ntot))\n",
    "        info_leak = Hc / H\n",
    "\n",
    "        inds = range(1, self.Ntot)\n",
    "        Nt = self.Nt\n",
    "\n",
    "        # ========== Helper for parallel computing ==========\n",
    "        def compute_Is(j_tuple):\n",
    "            \"\"\"Compute Is[j] for given agent combination j.\"\"\"\n",
    "            if max_combs is None:\n",
    "                noj = tuple(set(inds) - set(j_tuple))\n",
    "                p_a = p.sum(axis=(0, *noj), keepdims=True)\n",
    "                p_as = p.sum(axis=noj, keepdims=True)\n",
    "            else:\n",
    "                axis_keep = (0,) + j_tuple\n",
    "                axis_sum = tuple(set(range(self.Ntot)) - set(axis_keep))\n",
    "                p_as = p.sum(axis=axis_sum, keepdims=True)\n",
    "                p_a = p.sum(axis=tuple(set(range(1, self.Ntot)) - set(j_tuple)), keepdims=True)\n",
    "            p_s = p.sum(axis=tuple(range(1, self.Ntot)), keepdims=True)\n",
    "            p_a_s = p_as / p_s\n",
    "            p_s_a = p_as / p_a\n",
    "            return j_tuple, (p_a_s * (self.mylog(p_s_a) - self.mylog(p_s))).sum(axis=j_tuple).ravel()\n",
    "\n",
    "        # ========== Compute Is (parallelized) ==========\n",
    "        if max_combs is None:\n",
    "            comb_list = [j for i in inds for j in list(icmb(inds, i))]\n",
    "        else:\n",
    "            comb_list = [j for i in range(1, max_combs + 1) for j in list(icmb(inds, i))]\n",
    "\n",
    "        results = Parallel(n_jobs=n_jobs, backend=backend)(\n",
    "            delayed(compute_Is)(j) for j in comb_list\n",
    "        )\n",
    "        Is = dict(results)\n",
    "        #Is = {k: v for k, v in results if k is not None and v is not None}\n",
    "        p_s = p.sum(axis=tuple(range(1, self.Ntot)), keepdims=True)\n",
    "        MI = {k: (Is[k] * p_s.squeeze()).sum() for k in Is.keys()}\n",
    "\n",
    "        # ========== Initialize I_R, I_S ==========\n",
    "        if max_combs is None:\n",
    "            I_R = {cc: 0 for cc in comb_list}\n",
    "            I_S = {cc: 0 for cc in comb_list[self.Nvars:]}\n",
    "        else:\n",
    "            red_combs = []\n",
    "            for i in range(1, max_combs + 1):\n",
    "                for j in list(icmb(inds, i)):\n",
    "                    red_combs.append(j)\n",
    "            I_R = {cc: 0 for cc in red_combs}\n",
    "            I_S = {cc: 0 for cc in comb_list if len(cc) > 1}\n",
    "\n",
    "        # ========== Distribute MI to I_R / I_S ==========\n",
    "        for t in range(Nt):\n",
    "            I1 = np.array([ii[t] for ii in Is.values()])\n",
    "            i1 = np.argsort(I1)\n",
    "            lab = [list(comb_list[i_]) for i_ in i1]\n",
    "            lens = np.array([len(l) for l in lab])\n",
    "            I1 = I1[i1]\n",
    "\n",
    "            for l in range(1, lens.max()):\n",
    "                inds_l2 = np.where(lens == l + 1)[0]\n",
    "                Il1max = I1[lens == l].max()\n",
    "                inds_ = inds_l2[I1[inds_l2] < Il1max]\n",
    "                I1[inds_] = 0\n",
    "\n",
    "            i1 = np.argsort(I1)\n",
    "            lab = [lab[i_] for i_ in i1]\n",
    "            Di = np.diff(I1[i1], prepend=0.0)\n",
    "            red_vars = list(inds)\n",
    "            \n",
    "            for i_, ll in enumerate(lab):\n",
    "                info = Di[i_] * p_s.squeeze()[t]\n",
    "                if len(ll) == 1:\n",
    "                    I_R[tuple(red_vars)] += info\n",
    "                    if ll[0] in red_vars:\n",
    "                        red_vars.remove(ll[0])\n",
    "                else:\n",
    "                    I_S[tuple(ll)] += info\n",
    "\n",
    "        return I_R, I_S, MI, info_leak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17c7e1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Redundant (R):\n",
      "        (1, 2)      : 1.0000\n",
      "    Unique (U):\n",
      "        (1,)        : 0.0000\n",
      "        (2,)        : 0.0000\n",
      "    Synergystic (S):\n",
      "        (1, 2)      : 0.0000\n",
      "    Information Leak:  0.00%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "N = int( 1e7 )\n",
    "dt = 1\n",
    "nbins = 2\n",
    "    \n",
    "q1 = np.random.rand( N ).round().astype(int)\n",
    "q2 = np.random.rand( N ).round().astype(int)\n",
    "# Example 1: Duplicated input\n",
    "s = np.roll( q1, dt)\n",
    "a = ( q1, q1 )\n",
    "\n",
    "V = np.vstack([ s[dt:], [ a[i][:-dt] for i in range(len(a)) ] ]).T\n",
    "\n",
    "ic = InfoCausality(V, nbins=nbins)\n",
    "I_R, I_S, MI, info_leak = ic.surd()\n",
    "\n",
    "def print_surd( r_, s_, mi_, leak_ ):\n",
    "    '''Print the normalized redundancies, unique and synergy particles'''\n",
    "\n",
    "    r_ = {key: value / max(mi_.values()) for key, value in r_.items()}\n",
    "    s_ = {key: value / max(mi_.values()) for key, value in s_.items()}\n",
    "\n",
    "    print( '    Redundant (R):' )\n",
    "    for k_, v_ in r_.items():\n",
    "        if len(k_) > 1:\n",
    "            print( f'        {str(k_):12s}: {v_:5.4f}' )\n",
    "\n",
    "    print( '    Unique (U):' )\n",
    "    for k_, v_ in r_.items():\n",
    "        if len(k_) == 1:\n",
    "            print( f'        {str(k_):12s}: {v_:5.4f}' )\n",
    "\n",
    "    print( '    Synergystic (S):' )\n",
    "    for k_, v_ in s_.items():\n",
    "        print( f'        {str(k_):12s}: {v_:5.4f}' )\n",
    "\n",
    "    print(f'    Information Leak: {leak_ * 100:5.2f}%')\n",
    "    \n",
    "print_surd( I_R, I_S, MI, info_leak )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55511869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Configure matplotlib to use LaTeX for text rendering and set font size\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36b714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14fd3d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_R, I_S, MI, info_leak = ic.surd(n_jobs=2)\n",
    "I_R\n",
    "I_S\n",
    "info_leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ff1419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1,): 0, (2,): np.float64(0.0), (1, 2): np.float64(0.999999903634983)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c11ecc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2): np.float64(0.0)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa45b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pfm( s, a, dt, bins ):\n",
    "    '''compute the joint PMF of variables\n",
    "        \n",
    "        pfm = create_pfm( s, a, dt )\n",
    "    Parameters\n",
    "        s:      [np.ndarray]\n",
    "            temporal evolution of target variable [size (Nt,)]\n",
    "        a:      [ tuple ]\n",
    "            each element must be an np.ndarray of size (Nt,)\n",
    "            with the temporal evolution of agent variables\n",
    "        dt:     [int]\n",
    "            time lag in number of time steps\n",
    "        bins:     [int]\n",
    "            number of bins (states) per dimension\n",
    "    Returns\n",
    "        pfm     [np.ndarray]\n",
    "            Mass probability function ( size ( bins, ..., bins ), dim = 1 + len(a) )\n",
    "    \n",
    "    '''\n",
    "    V = np.vstack([ s[dt:], [ a[i][:-dt] for i in range(len(a)) ] ]).T\n",
    "    \n",
    "    # Histogram computes the bins by equally splitting the interval max(var)-min(var)\n",
    "    h, _ = np.histogramdd( V, bins=bins )\n",
    "    return h/h.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c77c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.49981725 0.        ]\n",
      "  [0.         0.        ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.50018275]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "N = int( 1e7 )\n",
    "dt = 1\n",
    "nbins = 2\n",
    "    \n",
    "q1 = np.random.rand( N ).round().astype(int)\n",
    "q2 = np.random.rand( N ).round().astype(int)\n",
    "# Example 1: Duplicated input\n",
    "s = np.roll( q1, dt)\n",
    "a = ( q1, q1 )\n",
    "nvars = 2\n",
    "\n",
    "V = np.vstack([ s[dt:], [ a[i][:-dt] for i in range(len(a)) ] ]).T\n",
    "\n",
    "hist = create_pfm(target, agents, dt, nbins)\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0f399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5083f2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.49981725, 0.50018275]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.sum(axis = (0,1))\n",
    "hist.sum(axis = (0,1),keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb816ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 1.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist / hist.sum(axis = (0,1),keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b08688f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.49981725, 0.        ],\n",
       "        [0.        , 0.50018275]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.sum(axis = 0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce965a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f56be97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c35802ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49981725, 0.        ],\n",
       "       [0.        , 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a2c588b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.        , 0.50018275]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d02764",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tuple(set(range(3)) - set([0,1,2])):\n",
    "    print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1635df5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geocompy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
