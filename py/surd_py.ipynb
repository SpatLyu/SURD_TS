{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84e914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations as icmb\n",
    "from typing import Tuple, Dict, Optional\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "class InfoCausality:\n",
    "    \"\"\"\n",
    "    InfoCausality Class for Information-Theoretic Causality Analysis.\n",
    "    Supports:\n",
    "        - Shannon entropy\n",
    "        - Mutual information\n",
    "        - Conditional mutual information\n",
    "        - Transfer entropy\n",
    "        - SURD decomposition (Synergy, Unique, Redundancy)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x: np.ndarray, nbins: int = 8):\n",
    "        \"\"\"\n",
    "        Initialize InfoCausality Class from raw target-agents original-lagged data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            2D array where:\n",
    "                - First column is the target variable (future)\n",
    "                - Remaining columns are agent (predictor) variables.\n",
    "        nbins : int\n",
    "            Number of bins (states) per variable dimension for discretization.\n",
    "        \"\"\"\n",
    "        self.p = self.create_pfm(x, nbins)\n",
    "        self.Ntot = self.p.ndim\n",
    "        self.Nvars = self.Ntot - 1\n",
    "        self.Nt = self.p.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def create_pfm(x: np.ndarray, nbins: int) -> np.ndarray:\n",
    "        x = x[~np.isnan(x).any(axis=1)]\n",
    "        if not len(x): raise ValueError(\"All values are NaN in input array.\")\n",
    "        hist, _ = np.histogramdd(x, bins=nbins)\n",
    "        hist = np.maximum(hist, 1e-14) #hist += 1e-14 \n",
    "        hist /= hist.sum()\n",
    "        return hist\n",
    "\n",
    "    @staticmethod\n",
    "    def mylog(x: np.ndarray) -> np.ndarray:\n",
    "        valid = (x > 0) & np.isfinite(x)\n",
    "        res = np.zeros_like(x)\n",
    "        res[valid] = np.log2(x[valid])\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(p: np.ndarray) -> float:\n",
    "        return -np.sum(p * InfoCausality.mylog(p))\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy_nvars(p: np.ndarray, indices) -> float:\n",
    "        excluded = tuple(set(range(p.ndim)) - set(indices))\n",
    "        marginalized = p if not excluded else p.sum(axis=excluded)\n",
    "        return InfoCausality.entropy(marginalized)\n",
    "\n",
    "    @staticmethod\n",
    "    def cond_entropy(p: np.ndarray, target_indices, conditioning_indices) -> float:\n",
    "        joint = InfoCausality.entropy_nvars(p, tuple(set(target_indices) | set(conditioning_indices)))\n",
    "        cond = InfoCausality.entropy_nvars(p, conditioning_indices)\n",
    "        return joint - cond\n",
    "\n",
    "    @staticmethod\n",
    "    def mutual_info(p: np.ndarray, set1_indices, set2_indices) -> float:\n",
    "        h1 = InfoCausality.entropy_nvars(p, set1_indices)\n",
    "        cond_h = InfoCausality.cond_entropy(p, set1_indices, set2_indices)\n",
    "        return h1 - cond_h\n",
    "\n",
    "    @staticmethod\n",
    "    def cond_mutual_info(p: np.ndarray, ind1, ind2, ind3) -> float:\n",
    "        combined = tuple(set(ind2) | set(ind3))\n",
    "        return InfoCausality.cond_entropy(p, ind1, ind3) - InfoCausality.cond_entropy(p, ind1, combined)\n",
    "\n",
    "    def transfer_entropy(self) -> np.ndarray:\n",
    "        num_vars = self.Nvars\n",
    "        TE = np.zeros(num_vars)\n",
    "        for i in range(1, num_vars + 1):\n",
    "            present_indices = tuple(range(1, num_vars + 1))\n",
    "            conditioning_indices = tuple(j for j in range(1, num_vars + 1) if j != i)\n",
    "            cond_ent_past = InfoCausality.cond_entropy(self.p, (0,), conditioning_indices)\n",
    "            cond_ent_past_input = InfoCausality.cond_entropy(self.p, (0,), present_indices)\n",
    "            TE[i - 1] = cond_ent_past - cond_ent_past_input\n",
    "        return TE\n",
    "\n",
    "    # =====================================================\n",
    "    # SURD decomposition (with parallel computation support)\n",
    "    # =====================================================\n",
    "    def surd(\n",
    "        self,\n",
    "        max_combs: Optional[int] = None,\n",
    "        n_jobs: int = 1,\n",
    "        backend: str = \"loky\",\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Compute unified SURD decomposition, optionally with parallelization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_combs : int or None\n",
    "            Maximum combination order for high-dimensional synergy computation.\n",
    "            If None, standard SURD decomposition is used.\n",
    "        n_jobs : int\n",
    "            Number of parallel jobs (default 1 = no parallel).\n",
    "        backend : str\n",
    "            Joblib backend: 'loky', 'threading', or 'multiprocessing'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            {\n",
    "                \"synergistic\": { \"X1-X2\": value, ... },\n",
    "                \"unique\": { \"X1\": value, \"X2\": value, ... },\n",
    "                \"redundant\": { \"X1-X2\": value, ... },\n",
    "                \"mutual_info\": { \"X1-X2\": value, ... },\n",
    "                \"info_leak\": float\n",
    "            }\n",
    "        \"\"\"\n",
    "        p = self.p\n",
    "        H = self.entropy_nvars(p, (0,))\n",
    "        Hc = self.cond_entropy(p, (0,), range(1, self.Ntot))\n",
    "        info_leak = Hc / H\n",
    "\n",
    "        inds = range(1, self.Ntot)\n",
    "        Nt = self.Nt\n",
    "\n",
    "        # ========== Helper for parallel computing ==========\n",
    "        def compute_Is(j_tuple):\n",
    "            \"\"\"Compute Is[j] for given agent combination j.\"\"\"\n",
    "            if max_combs is None:\n",
    "                noj = tuple(set(inds) - set(j_tuple))\n",
    "                p_a = p.sum(axis=(0, *noj), keepdims=True)\n",
    "                p_as = p.sum(axis=noj, keepdims=True)\n",
    "            else:\n",
    "                axis_keep = (0,) + j_tuple\n",
    "                axis_sum = tuple(set(range(self.Ntot)) - set(axis_keep))\n",
    "                p_as = p.sum(axis=axis_sum, keepdims=True)\n",
    "                p_a = p.sum(axis=tuple(set(range(1, self.Ntot)) - set(j_tuple)), keepdims=True)\n",
    "            p_s = p.sum(axis=tuple(range(1, self.Ntot)), keepdims=True)\n",
    "            p_a_s = p_as / p_s\n",
    "            p_s_a = p_as / p_a\n",
    "            return j_tuple, (p_a_s * (self.mylog(p_s_a) - self.mylog(p_s))).sum(axis=j_tuple).ravel()\n",
    "\n",
    "        # ========== Compute Is (parallelized) ==========\n",
    "        if max_combs is None:\n",
    "            comb_list = [j for i in inds for j in icmb(inds, i)]\n",
    "        else:\n",
    "            comb_list = [j for i in range(1, max_combs + 1) for j in icmb(inds, i)]\n",
    "\n",
    "        results = Parallel(n_jobs=n_jobs, backend=backend)(\n",
    "            delayed(compute_Is)(j) for j in comb_list\n",
    "        )\n",
    "        Is = dict(results)\n",
    "        p_s = p.sum(axis=tuple(range(1, self.Ntot)), keepdims=True)\n",
    "        MI = {k: (Is[k] * p_s.squeeze()).sum() for k in Is.keys()}\n",
    "\n",
    "        # ========== Initialize I_R, I_S ==========\n",
    "        if max_combs is None:\n",
    "            I_R = {cc: 0 for cc in comb_list}\n",
    "            I_S = {cc: 0 for cc in comb_list[self.Nvars:]}\n",
    "        else:\n",
    "            red_combs = [j for i in range(1, max_combs + 1) for j in icmb(inds, i)]\n",
    "            I_R = {cc: 0 for cc in red_combs}\n",
    "            I_S = {cc: 0 for cc in comb_list if len(cc) > 1}\n",
    "\n",
    "        # ========== Distribute MI to I_R / I_S ==========\n",
    "        for t in range(Nt):\n",
    "            I1 = np.array([ii[t] for ii in Is.values()])\n",
    "            i1 = np.argsort(I1)\n",
    "            lab = [list(comb_list[i_]) for i_ in i1]\n",
    "            lens = np.array([len(l) for l in lab])\n",
    "            I1 = I1[i1]\n",
    "\n",
    "            for l in range(1, lens.max()):\n",
    "                inds_l2 = np.where(lens == l + 1)[0]\n",
    "                Il1max = I1[lens == l].max()\n",
    "                inds_ = inds_l2[I1[inds_l2] < Il1max]\n",
    "                I1[inds_] = 0\n",
    "\n",
    "            i1 = np.argsort(I1)\n",
    "            lab = [lab[i_] for i_ in i1]\n",
    "            Di = np.diff(I1[i1], prepend=0.0)\n",
    "            red_vars = list(inds)\n",
    "            \n",
    "            for i_, ll in enumerate(lab):\n",
    "                info = Di[i_] * p_s.squeeze()[t]\n",
    "                if len(ll) == 1:\n",
    "                    I_R[tuple(red_vars)] += info\n",
    "                    if ll[0] in red_vars:\n",
    "                        red_vars.remove(ll[0])\n",
    "                else:\n",
    "                    I_S[tuple(ll)] += info\n",
    "                    \n",
    "        # =========================================================\n",
    "        # Format names: X1, X2, X1-X2-X3\n",
    "        # =========================================================\n",
    "        varnames = [f\"X{i}\" for i in range(1, self.Ntot)]\n",
    "        def combo_name(tup):\n",
    "            return \"-\".join(varnames[i - 1] for i in tup)\n",
    "\n",
    "        # ======== Convert keys to readable names ========\n",
    "        I_R_named = {combo_name(k): v for k, v in I_R.items()}\n",
    "        I_S_named = {combo_name(k): v for k, v in I_S.items()}\n",
    "        MI_named  = {combo_name(k): v for k, v in MI.items()}\n",
    "\n",
    "        # ======== Split redundant / unique ========\n",
    "        unique_named = {k: v for k, v in I_R_named.items() if \"-\" not in k}\n",
    "        redundant_named = {k: v for k, v in I_R_named.items() if \"-\" in k}\n",
    "\n",
    "        # ======== Normalize for printing ========\n",
    "        max_mi = max(MI_named.values()) if MI_named else 1.0\n",
    "        norm = lambda d: {k: v / max_mi for k, v in d.items()}\n",
    "\n",
    "        print(\"\\nSURD Decomposition Results:\")\n",
    "        print(\"  Redundant (R):\")\n",
    "        for k_, v_ in norm(redundant_named).items():\n",
    "            print(f\"    {k_:15s}: {v_:6.4f}\")\n",
    "        print(\"  Unique (U):\")\n",
    "        for k_, v_ in norm(unique_named).items():\n",
    "            print(f\"    {k_:15s}: {v_:6.4f}\")\n",
    "        print(\"  Synergistic (S):\")\n",
    "        for k_, v_ in norm(I_S_named).items():\n",
    "            print(f\"    {k_:15s}: {v_:6.4f}\")\n",
    "        print(f\"  Information Leak: {info_leak * 100:6.2f}%\\n\")\n",
    "\n",
    "        # ======== Return friendly structured dict ========\n",
    "        result = {\n",
    "            \"synergistic\": I_S_named,\n",
    "            \"unique\": unique_named,\n",
    "            \"redundant\": redundant_named,\n",
    "            \"mutual_info\": MI_named,\n",
    "            \"info_leak\": info_leak,\n",
    "        }\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e922fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SURD Decomposition Results:\n",
      "  Redundant (R):\n",
      "    X1-X2          : 1.0000\n",
      "  Unique (U):\n",
      "    X1             : 0.0000\n",
      "    X2             : 0.0000\n",
      "  Synergistic (S):\n",
      "    X1-X2          : 0.0000\n",
      "  Information Leak:   0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "N = int( 1e7 )\n",
    "dt = 1\n",
    "nbins = 2\n",
    "    \n",
    "q1 = np.random.rand( N ).round().astype(int)\n",
    "q2 = np.random.rand( N ).round().astype(int)\n",
    "# Example 1: Duplicated input\n",
    "s = np.roll( q1, dt)\n",
    "a = ( q1, q1 )\n",
    "\n",
    "V = np.vstack([ s[dt:], [ a[i][:-dt] for i in range(len(a)) ] ]).T\n",
    "\n",
    "ic = InfoCausality(V, nbins=nbins)\n",
    "a = ic.surd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geocompy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
