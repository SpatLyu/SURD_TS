{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84e914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations as icmb\n",
    "from typing import Tuple, Dict, Optional\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "class InfoCausality:\n",
    "    \"\"\"\n",
    "    InfoCausality class for Information-Theoretic Causality Analysis.\n",
    "    Supports:\n",
    "        - Shannon entropy\n",
    "        - Mutual information\n",
    "        - Conditional mutual information\n",
    "        - Transfer entropy\n",
    "        - SURD decomposition (Synergy, Unique, Redundancy)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x: np.ndarray, nbins: int = 8):\n",
    "        \"\"\"\n",
    "        Initialize InfoCausality class from raw target-agents original-lagged data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            2D array where:\n",
    "                - First column is the target variable (future)\n",
    "                - Remaining columns are agent (predictor) variables.\n",
    "        nbins : int\n",
    "            Number of bins (states) per variable dimension for discretization.\n",
    "        \"\"\"\n",
    "        self.p = self.create_pfm(x, nbins)\n",
    "        self.Ntot = self.p.ndim\n",
    "        self.Nvars = self.Ntot - 1\n",
    "        self.Nt = self.p.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def create_pfm(x: np.ndarray, nbins: int) -> np.ndarray:\n",
    "        \"\"\"Create joint probability frequency matrix.\"\"\"\n",
    "        hist, _ = np.histogramdd(x, bins=nbins)\n",
    "        hist = np.maximum(hist, 1e-14) #hist += 1e-14 \n",
    "        hist /= hist.sum()\n",
    "        return hist\n",
    "\n",
    "    @staticmethod\n",
    "    def mylog(x: np.ndarray) -> np.ndarray:\n",
    "        valid = (x > 0) & np.isfinite(x)\n",
    "        res = np.zeros_like(x)\n",
    "        res[valid] = np.log2(x[valid])\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(p: np.ndarray) -> float:\n",
    "        return -np.sum(p * InfoCausality.mylog(p))\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy_nvars(p: np.ndarray, indices) -> float:\n",
    "        excluded = tuple(set(range(p.ndim)) - set(indices))\n",
    "        marginalized = p if not excluded else p.sum(axis=excluded)\n",
    "        return InfoCausality.entropy(marginalized)\n",
    "\n",
    "    @staticmethod\n",
    "    def cond_entropy(p: np.ndarray, target_indices, conditioning_indices) -> float:\n",
    "        joint = InfoCausality.entropy_nvars(p, tuple(set(target_indices) | set(conditioning_indices)))\n",
    "        cond = InfoCausality.entropy_nvars(p, conditioning_indices)\n",
    "        return joint - cond\n",
    "\n",
    "    @staticmethod\n",
    "    def mutual_info(p: np.ndarray, set1_indices, set2_indices) -> float:\n",
    "        h1 = InfoCausality.entropy_nvars(p, set1_indices)\n",
    "        cond_h = InfoCausality.cond_entropy(p, set1_indices, set2_indices)\n",
    "        return h1 - cond_h\n",
    "\n",
    "    @staticmethod\n",
    "    def cond_mutual_info(p: np.ndarray, ind1, ind2, ind3) -> float:\n",
    "        combined = tuple(set(ind2) | set(ind3))\n",
    "        return InfoCausality.cond_entropy(p, ind1, ind3) - InfoCausality.cond_entropy(p, ind1, combined)\n",
    "\n",
    "    def transfer_entropy(self) -> np.ndarray:\n",
    "        num_vars = self.Nvars\n",
    "        TE = np.zeros(num_vars)\n",
    "        for i in range(1, num_vars + 1):\n",
    "            present_indices = tuple(range(1, num_vars + 1))\n",
    "            conditioning_indices = tuple(j for j in range(1, num_vars + 1) if j != i)\n",
    "            cond_ent_past = InfoCausality.cond_entropy(self.p, (0,), conditioning_indices)\n",
    "            cond_ent_past_input = InfoCausality.cond_entropy(self.p, (0,), present_indices)\n",
    "            TE[i - 1] = cond_ent_past - cond_ent_past_input\n",
    "        return TE\n",
    "\n",
    "    # =====================================================\n",
    "    # SURD decomposition (with parallel computation support)\n",
    "    # =====================================================\n",
    "    def surd(\n",
    "        self,\n",
    "        max_combs: Optional[int] = None,\n",
    "        n_jobs: int = 1,\n",
    "        backend: str = \"loky\",\n",
    "    ) -> Tuple[Dict, Dict, Dict, float]:\n",
    "        \"\"\"\n",
    "        Compute unified SURD decomposition, optionally with parallelization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_combs : int or None\n",
    "            Maximum combination order for high-dimensional synergy computation.\n",
    "            If None, standard SURD decomposition is used.\n",
    "        n_jobs : int\n",
    "            Number of parallel jobs (default 1 = no parallel).\n",
    "        backend : str\n",
    "            Joblib backend: 'loky', 'threading', or 'multiprocessing'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        I_R : dict\n",
    "            Redundancy / unique information for each combination.\n",
    "        I_S : dict\n",
    "            Synergy information for each combination.\n",
    "        MI : dict\n",
    "            Mutual information for each combination.\n",
    "        info_leak : float\n",
    "            Information leak ratio Hc/H.\n",
    "        \"\"\"\n",
    "        p = self.p\n",
    "        H = self.entropy_nvars(p, (0,))\n",
    "        Hc = self.cond_entropy(p, (0,), range(1, self.Ntot))\n",
    "        info_leak = Hc / H\n",
    "\n",
    "        inds = range(1, self.Ntot)\n",
    "        Nt = self.Nt\n",
    "\n",
    "        # ========== Helper for parallel computing ==========\n",
    "        def compute_Is(j_tuple):\n",
    "            \"\"\"Compute Is[j] for given agent combination j.\"\"\"\n",
    "            if max_combs is None:\n",
    "                noj = tuple(set(inds) - set(j_tuple))\n",
    "                p_a = p.sum(axis=(0, *noj), keepdims=True)\n",
    "                p_as = p.sum(axis=noj, keepdims=True)\n",
    "            else:\n",
    "                axis_keep = (0,) + j_tuple\n",
    "                axis_sum = tuple(set(range(self.Ntot)) - set(axis_keep))\n",
    "                p_as = p.sum(axis=axis_sum, keepdims=True)\n",
    "                p_a = p.sum(axis=tuple(set(range(1, self.Ntot)) - set(j_tuple)), keepdims=True)\n",
    "            p_s = p.sum(axis=tuple(range(1, self.Ntot)), keepdims=True)\n",
    "            p_a_s = p_as / p_s\n",
    "            p_s_a = p_as / p_a\n",
    "            return j_tuple, (p_a_s * (self.mylog(p_s_a) - self.mylog(p_s))).sum(axis=j_tuple).ravel()\n",
    "\n",
    "        # ========== Compute Is (parallelized) ==========\n",
    "        if max_combs is None:\n",
    "            comb_list = [j for i in inds for j in icmb(inds, i)]\n",
    "        else:\n",
    "            comb_list = [j for i in range(1, max_combs + 1) for j in icmb(inds, i)]\n",
    "\n",
    "        results = Parallel(n_jobs=n_jobs, backend=backend)(\n",
    "            delayed(compute_Is)(j) for j in comb_list\n",
    "        )\n",
    "        Is = dict(results)\n",
    "        p_s = p.sum(axis=tuple(range(1, self.Ntot)), keepdims=True)\n",
    "        MI = {k: (Is[k] * p_s.squeeze()).sum() for k in Is.keys()}\n",
    "\n",
    "        # ========== Initialize I_R, I_S ==========\n",
    "        if max_combs is None:\n",
    "            I_R = {cc: 0 for cc in comb_list}\n",
    "            I_S = {cc: 0 for cc in comb_list[self.Nvars:]}\n",
    "        else:\n",
    "            red_combs = [j for i in range(1, max_combs + 1) for j in icmb(inds, i)]\n",
    "            I_R = {cc: 0 for cc in red_combs}\n",
    "            I_S = {cc: 0 for cc in comb_list if len(cc) > 1}\n",
    "\n",
    "        # ========== Distribute MI to I_R / I_S ==========\n",
    "        for t in range(Nt):\n",
    "            I1 = np.array([ii[t] for ii in Is.values()])\n",
    "            i1 = np.argsort(I1)\n",
    "            lab = [list(comb_list[i_]) for i_ in i1]\n",
    "            lens = np.array([len(l) for l in lab])\n",
    "            I1 = I1[i1]\n",
    "\n",
    "            for l in range(1, lens.max()):\n",
    "                inds_l2 = np.where(lens == l + 1)[0]\n",
    "                Il1max = I1[lens == l].max()\n",
    "                inds_ = inds_l2[I1[inds_l2] < Il1max]\n",
    "                I1[inds_] = 0\n",
    "\n",
    "            i1 = np.argsort(I1)\n",
    "            lab = [lab[i_] for i_ in i1]\n",
    "            Di = np.diff(I1[i1], prepend=0.0)\n",
    "            red_vars = list(inds)\n",
    "            \n",
    "            for i_, ll in enumerate(lab):\n",
    "                info = Di[i_] * p_s.squeeze()[t]\n",
    "                if len(ll) == 1:\n",
    "                    I_R[tuple(red_vars)] += info\n",
    "                    if ll[0] in red_vars:\n",
    "                        red_vars.remove(ll[0])\n",
    "                else:\n",
    "                    I_S[tuple(ll)] += info\n",
    "\n",
    "        return I_R, I_S, MI, info_leak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c7e1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Redundant (R):\n",
      "        (1, 2)      : 1.0000\n",
      "    Unique (U):\n",
      "        (1,)        : 0.0000\n",
      "        (2,)        : 0.0000\n",
      "    Synergystic (S):\n",
      "        (1, 2)      : 0.0000\n",
      "    Information Leak:  0.00%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "N = int( 1e7 )\n",
    "dt = 1\n",
    "nbins = 2\n",
    "    \n",
    "q1 = np.random.rand( N ).round().astype(int)\n",
    "q2 = np.random.rand( N ).round().astype(int)\n",
    "# Example 1: Duplicated input\n",
    "s = np.roll( q1, dt)\n",
    "a = ( q1, q1 )\n",
    "\n",
    "V = np.vstack([ s[dt:], [ a[i][:-dt] for i in range(len(a)) ] ]).T\n",
    "\n",
    "ic = InfoCausality(V, nbins=nbins)\n",
    "I_R, I_S, MI, info_leak = ic.surd()\n",
    "\n",
    "def print_surd( r_, s_, mi_, leak_ ):\n",
    "    '''Print the normalized redundancies, unique and synergy particles'''\n",
    "\n",
    "    r_ = {key: value / max(mi_.values()) for key, value in r_.items()}\n",
    "    s_ = {key: value / max(mi_.values()) for key, value in s_.items()}\n",
    "\n",
    "    print( '    Redundant (R):' )\n",
    "    for k_, v_ in r_.items():\n",
    "        if len(k_) > 1:\n",
    "            print( f'        {str(k_):12s}: {v_:5.4f}' )\n",
    "\n",
    "    print( '    Unique (U):' )\n",
    "    for k_, v_ in r_.items():\n",
    "        if len(k_) == 1:\n",
    "            print( f'        {str(k_):12s}: {v_:5.4f}' )\n",
    "\n",
    "    print( '    Synergystic (S):' )\n",
    "    for k_, v_ in s_.items():\n",
    "        print( f'        {str(k_):12s}: {v_:5.4f}' )\n",
    "\n",
    "    print(f'    Information Leak: {leak_ * 100:5.2f}%')\n",
    "    \n",
    "print_surd( I_R, I_S, MI, info_leak )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1635df5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geocompy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
